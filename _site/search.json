[
  {
    "objectID": "posts/TMDB_scraper/index.html",
    "href": "posts/TMDB_scraper/index.html",
    "title": "Web scraping with Scrapy",
    "section": "",
    "text": "Scraping has become an essential skill for data enthusiasts, researchers, and developers. Today, we’ll explore how to extract movie and actor data from The Movie Database (TMDb) using Scrapy, a popular Python web scraping framework. Our goal would be to scrape data about movies that share actors with a certain movie of our choice. Specifically, we’ll start with the page of a particular movie, navigate to its cast page, and then visit each actor’s page to get the list of movies they starred."
  },
  {
    "objectID": "posts/TMDB_scraper/index.html#part-i-lets-start-scraping",
    "href": "posts/TMDB_scraper/index.html#part-i-lets-start-scraping",
    "title": "Web scraping with Scrapy",
    "section": "Part I: Let’s start scraping!",
    "text": "Part I: Let’s start scraping!\nA note before start:\n\nFor now, add CLOSESPIDER_PAGECOUNT = 20 to the file settings.py. This line just prevents your scraper from downloading too much data while you’re still testing things out. You’ll remove this line later.\nYou may run into 403 Forbidden errors once the website detects that you’re a bot. One of the easiest solution is to add the following line to the same file:\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\n\n\nStep0 : Start a Scrapy project\nAfter pip install the package, start a new project with the following command:\n&gt;conda activate PIC16B-2\n&gt;scrapy startproject TMDB_scraper\n&gt;cd TMDB_scraper\nThis will create a folder with necessary files.\n\n\nStep1: Define a spider class\nPick your favorite movie or TV show, and locate its TMDB page by searching on https://www.themoviedb.org/. Here I chose Léon: The Professional as an example. The we will start by creating a spider class which defines how to follow links and extract data. In the code, the name attribute is a unique identifier for the spider, and start_urls contains the initial URL(s) the spider will begin scraping from.\n\nclass TmdbSpider(scrapy.Spider):\n    \"\"\"\n    A Scrapy Spider class to scrape movie and actor data from The Movie Database (TMDb).\n\n    Attributes:\n        name (str): The name of the spider.\n        start_urls (list): A list containing the initial URL to start scraping.\n    \"\"\"\n\n    # Create a spider class, and assign a name to the spider\n    name = 'tmdb_spider'\n    start_urls = ['https://www.themoviedb.org/movie/101-leon-the-professional']\n\n\n\nStep2: Locate the Starting TMDB Page\nWe will first define a parse(self, response) method to start on the movie page, and then navigate to the Full Cast & Crew page, which has url of the form &lt;movie_url&gt;cast.\n\ndef parse(self, response):\n    \"\"\"\n    Parses the main movie page and navigates to the cast page.\n\n    Args:\n        response (obj): The response object containing the scraped data.\n\n    Yields:\n        scrapy.Request: A new request to the cast URL.\n    \"\"\"\n    full_credits_url = f'{response.url}/cast' # hardcode the url format\n    yield scrapy.Request(full_credits_url, callback=self.parse_full_credits) # Yield a request to the cast URL\n\n\n\nStep3: Locate the actor’s page\nHere we will define a method called parse_full_credits(self, response) to extract actor links from the cast page and navigate to each actor’s page when we are at the cast page after step2.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parses the cast page, extracting actor links and navigating to each actor's page.\n\n    Args:\n        response (obj): The response object containing the scraped data.\n\n    Yields:\n        scrapy.Request: A new request to each actor's URL.\n    \"\"\"\n    # Extract all links to actor's page\n    actor_links = response.css('ol.people.credits:not(.crew) div a::attr(href)').extract()\n    # Iterate through the links and yield request to each\n    pref = \"https://www.themoviedb.org\"\n    for actor_link in actor_links:\n        yield scrapy.Request(pref+actor_link, callback=self.parse_actor_page)\n\n\n\nStep4: Scrape actor’s name and a list of movies\nFinally we will define a parse_actor_page(self, response) method to interact with the page of an actor. It will yield a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}. The method will yield one such dictionary for each of the movies or TV shows on which that actor has worked.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses an actor's page, extracting the actor's name and the movies or TV shows they have participated in.\n\n    Args:\n        response (obj): The response object containing the scraped data.\n\n    Yields:\n        dict: A dictionary containing the actor's name and the name of a movie or TV show they have participated in.\n    \"\"\"\n    # Extract actor's name\n    actor_name = response.css('head title::text').extract_first().split(\" —\")[0]\n    # Extract a list of movies starred by this actor, and yield the dictionary\n    movies = response.css('td.role.true.account_adult_false.item_adult_false bdi::text').extract()\n    for movie in movies:\n        yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n\n\n\nStep5: Run the spider\nA csv file will be created to store scraped data using command scrapy crawl tmdb_spider -o &lt;csv_name&gt;.csv"
  },
  {
    "objectID": "posts/TMDB_scraper/index.html#part-ii-visualize-the-reult",
    "href": "posts/TMDB_scraper/index.html#part-ii-visualize-the-reult",
    "title": "Web scraping with Scrapy",
    "section": "Part II: Visualize the reult",
    "text": "Part II: Visualize the reult\n\n# For successful rendering of the polt\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport pandas as pd\nimport plotly.express as px\n\n# Read in scraped data\ndf = pd.read_csv('movies.csv')\n\n\n# Take a look at the data \ndf.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPeter Linari\nHustle\n\n\n1\nPeter Linari\nGood Time\n\n\n2\nPeter Linari\nStraighten Up and Fly Right\n\n\n3\nPeter Linari\nSeason of the Hunted\n\n\n4\nPeter Linari\nThe Curse of the Jade Scorpion\n\n\n\n\n\n\n\n\n# Get the top10 movies starred by same actors in our favorite movie \ntop10 = df.groupby('movie_or_TV_name')['actor'].count().sort_values(ascending = False)[1:11]\ntop10 = pd.DataFrame(top10).reset_index() # Convert to dataframe\n\n\n# Plot a bar chart, and add title\nfig = px.bar(top10, x='movie_or_TV_name', y='actor')\nfig.update_layout(title_text=\"Top 10 movies or TV shows with most shared actors\")\n\nfig.show()\n\n\n\n\nFrom this bar plot we can see Law & Order has the most shared actors with Léon: The Professional."
  },
  {
    "objectID": "posts/Dog Cat classification/Dog_cat_classfication.html",
    "href": "posts/Dog Cat classification/Dog_cat_classfication.html",
    "title": "Dog Cat Classfication with Tensorflow",
    "section": "",
    "text": "Can you distinguish between pictures of dogs and pictures of cats using machine learning algorithms?\nIn this blog, we will be using Tensorflow to build several neural network models to classify dogs and cats. In this tutorial, you will be familiarized with image data augmentation, preprocessing, and the concept of transfer learning. It is strongly recommended to use colab or any other cloud services to access GPUs for faster training."
  },
  {
    "objectID": "posts/Dog Cat classification/Dog_cat_classfication.html#data-preparation",
    "href": "posts/Dog Cat classification/Dog_cat_classfication.html#data-preparation",
    "title": "Dog Cat Classfication with Tensorflow",
    "section": "Data Preparation",
    "text": "Data Preparation\nLet’s start with the data import. We will be using a labeled dataset provided by Tensorflow, and we can retrive it from a url.\n\n# package import\nimport os\nfrom tensorflow.keras import utils\nimport tensorflow as tf\n\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets\ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                          shuffle=True,\n                          batch_size=BATCH_SIZE,\n                          image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                            shuffle=True,\n                            batch_size=BATCH_SIZE,\n                            image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\nBy running this code, we have created TensorFlow Datasets instances for training, validation, and testing. The data will be feeded to the machine learning in batches, and this avoids the need to load all the data into memory.\nIn our case, we used a special-purpose keras utility called image_dataset_from_directory to construct a Dataset. The first arguments specifies the directory of the dataset. The shuffle argument says that, when retrieving data from this directory, the order should be randomized. The batch_size determines how many data points are gathered from the directory at once. Here, for example, each time we request some data we will get 32 images from each of the data sets. Finally, the image_size specifies the size of the input images.\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\nThis is technical code related to rapidly reading data. Here for more\nWe will be using take method to interact with the dataset. For example, train_dataset.take(1) will retrieve one batch (32 images with labels) from the training data. We can write a simple function to visualize some random pictures in the dataset.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef show_random_pictures(train_dataset, class_names):\n    plt.figure(figsize=(10, 5))\n    cat_images = []\n    dog_images = []\n\n    # Take 1 batches to ensure we get at least 3 of each class\n    for images, labels in train_dataset.take(1):\n        for i in range(len(labels)):\n            # Class 0 for Cat, Class 1 for Dog\n            if labels[i] == 0 and len(cat_images) &lt; 3:\n                cat_images.append(images[i].numpy().astype(\"uint8\"))\n            elif labels[i] == 1 and len(dog_images) &lt; 3:\n                dog_images.append(images[i].numpy().astype(\"uint8\"))\n            if len(cat_images) == 3 and len(dog_images) == 3:\n                break\n        if len(cat_images) == 3 and len(dog_images) == 3:\n            break\n\n    # Check if we found enough images\n    if len(cat_images) &lt; 3 or len(dog_images) &lt; 3:\n        raise ValueError(\"Not enough cat or dog images found in the batches taken.\")\n\n    # Plotting Cats\n    for i in range(3):\n        plt.subplot(2, 3, i+1)\n        plt.imshow(cat_images[i])\n        plt.title(class_names[0])\n        plt.axis('off')\n\n    # Plotting Dogs\n    for i in range(3):\n        plt.subplot(2, 3, i+4)\n        plt.imshow(dog_images[i])\n        plt.title(class_names[1])\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\nclass_names = ['Cat', 'Dog'] # specify class names\nshow_random_pictures(train_dataset, class_names)\n\n\n\n\n\ndef counter(labels_iterator):\n    # Initialize counters for both labels\n    count_label_0 = 0  # for cat\n    count_label_1 = 0  # for dog\n\n    for label in labels_iterator:\n        if label == 0:\n            count_label_0 += 1\n        elif label == 1:\n            count_label_1 += 1\n\n    # Print out the counts\n    print(f'Number of cat images: {count_label_0}')\n    print(f'Number of dog images: {count_label_1}')\n\n\n# The following line of code will create an iterator to iterate through the dataset\nlabels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncounter(labels_iterator)\n\nNumber of cat images: 1000\nNumber of dog images: 1000\n\n\nSince random guess will get 50% accuracy, we will treat this as the benchmark for improvement. Our models should do much better than a 50% baseline."
  },
  {
    "objectID": "posts/Dog Cat classification/Dog_cat_classfication.html#some-helper-functions",
    "href": "posts/Dog Cat classification/Dog_cat_classfication.html#some-helper-functions",
    "title": "Dog Cat Classfication with Tensorflow",
    "section": "Some Helper Functions",
    "text": "Some Helper Functions\nFor convenience, we will create two helper functions to help us generalize the training of different models and the visualization of results.\n\n# Function to compile and train a model using training and validation datasets.\ndef compile_and_train(model, train_dataset, validation_dataset):\n    # Compiling the model with 'adam' optimizer, loss function for multiclass classification, \n    # and setting 'accuracy' as the metric for evaluation.\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n    # Training the model on the training dataset for 20 epochs\n    # and validating it on validation dataset.\n    history = model.fit(\n        train_dataset,\n        epochs=20,\n        validation_data=validation_dataset\n    )\n\n    # Returning the history object containing details of training and validation performance.\n    return history\n\n# Function to visualize the training and validation accuracy over epochs.\ndef plot_training_history(history):\n    # Extracting accuracy metrics from the training history.\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    # Setting up the range of epochs for plotting.\n    epochs_range = range(20)\n\n    # Initializing a plot with a specific size.\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n\n    # Plotting the training accuracy and validation accuracy over epochs.\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n\n    # Adding legend to the plot in the lower right position.\n    plt.legend(loc='lower right')\n\n    # Adding a title to the plot.\n    plt.title('Training and Validation Accuracy')\n\n    # Displaying the plot.\n    plt.show()"
  },
  {
    "objectID": "posts/Dog Cat classification/Dog_cat_classfication.html#modeling",
    "href": "posts/Dog Cat classification/Dog_cat_classfication.html#modeling",
    "title": "Dog Cat Classfication with Tensorflow",
    "section": "Modeling",
    "text": "Modeling\n\n1. Simple CNN Model\nWe can start with a simple convolutional neural network model with three convolutional blocks and one classifier. The task is binary classification so we would define the last layer as tf.keras.layers.Dense(2) with two output neurons. tf.keras.layers.BatchNormalization() applies batch normalization to the inputs, which is a technique used to improve the performance and stability of the model.\n\n# Define the model\nmodel1 = tf.keras.Sequential([\n\n    # First Convolutional Block\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Second Convolutional Block\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Third Convolutional Block\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Classifier\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(2)\n])\n\n\nhistory1 = compile_and_train(model1, train_dataset, validation_dataset)\n\nEpoch 1/20\n63/63 [==============================] - 32s 154ms/step - loss: 1.2226 - accuracy: 0.5705 - val_loss: 3.6936 - val_accuracy: 0.5087\nEpoch 2/20\n63/63 [==============================] - 8s 122ms/step - loss: 0.9374 - accuracy: 0.5825 - val_loss: 0.7353 - val_accuracy: 0.6027\nEpoch 3/20\n63/63 [==============================] - 8s 127ms/step - loss: 0.7614 - accuracy: 0.6440 - val_loss: 0.7294 - val_accuracy: 0.6015\nEpoch 4/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.6735 - accuracy: 0.6870 - val_loss: 0.8700 - val_accuracy: 0.5804\nEpoch 5/20\n63/63 [==============================] - 8s 122ms/step - loss: 0.5658 - accuracy: 0.7295 - val_loss: 0.7950 - val_accuracy: 0.5792\nEpoch 6/20\n63/63 [==============================] - 9s 133ms/step - loss: 0.4795 - accuracy: 0.7855 - val_loss: 0.8196 - val_accuracy: 0.6423\nEpoch 7/20\n63/63 [==============================] - 9s 136ms/step - loss: 0.4845 - accuracy: 0.7840 - val_loss: 0.7983 - val_accuracy: 0.6163\nEpoch 8/20\n63/63 [==============================] - 8s 122ms/step - loss: 0.3717 - accuracy: 0.8355 - val_loss: 1.0909 - val_accuracy: 0.6089\nEpoch 9/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.3083 - accuracy: 0.8635 - val_loss: 0.7620 - val_accuracy: 0.6634\nEpoch 10/20\n63/63 [==============================] - 10s 149ms/step - loss: 0.2690 - accuracy: 0.8860 - val_loss: 0.7754 - val_accuracy: 0.6894\nEpoch 11/20\n63/63 [==============================] - 8s 122ms/step - loss: 0.2564 - accuracy: 0.8965 - val_loss: 0.7915 - val_accuracy: 0.6980\nEpoch 12/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.1821 - accuracy: 0.9245 - val_loss: 0.8409 - val_accuracy: 0.6770\nEpoch 13/20\n63/63 [==============================] - 8s 125ms/step - loss: 0.1559 - accuracy: 0.9400 - val_loss: 0.8358 - val_accuracy: 0.7129\nEpoch 14/20\n63/63 [==============================] - 8s 125ms/step - loss: 0.1368 - accuracy: 0.9505 - val_loss: 0.8645 - val_accuracy: 0.6918\nEpoch 15/20\n63/63 [==============================] - 8s 127ms/step - loss: 0.1086 - accuracy: 0.9605 - val_loss: 0.8438 - val_accuracy: 0.7351\nEpoch 16/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.1015 - accuracy: 0.9650 - val_loss: 0.9680 - val_accuracy: 0.6980\nEpoch 17/20\n63/63 [==============================] - 9s 129ms/step - loss: 0.1015 - accuracy: 0.9605 - val_loss: 0.9604 - val_accuracy: 0.6943\nEpoch 18/20\n63/63 [==============================] - 8s 127ms/step - loss: 0.0850 - accuracy: 0.9675 - val_loss: 1.1079 - val_accuracy: 0.6881\nEpoch 19/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.0867 - accuracy: 0.9680 - val_loss: 1.0985 - val_accuracy: 0.7042\nEpoch 20/20\n63/63 [==============================] - 8s 125ms/step - loss: 0.0789 - accuracy: 0.9685 - val_loss: 1.2521 - val_accuracy: 0.6881\n\n\n\nplot_training_history(history1)\n\n\n\n\nAs we can observe, the training accuracy goes all the way up to 0.97, while the validation accuracy fluctuates around 0.68. This is a clear signal of overfitting. But there is at least a 0.18 improvement over the baseline.\n\n\n2. Model with Data Augmentation\nNow we are going to add some data augmentation layers to the model. Data augmentation refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called invariant features of our input images. We will start by looking at the actual examples of flipping and rotation.\n\nRandomFlip Augmentation\n\ndef visualize_random_flip(image):\n    random_flip_layer = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n\n    plt.figure(figsize=(10, 10))\n    plt.subplot(3, 3, 1)\n    plt.imshow(image.numpy().astype(\"uint8\"))\n    plt.title('Original image')\n    plt.axis('off')\n    # Apply the RandomFlip layer to the same image several times\n    for i in range(8):\n        plt.subplot(3, 3, i+2)\n        flipped_image = random_flip_layer(image, training=True)\n        plt.imshow(flipped_image.numpy().astype(\"uint8\"))\n        plt.title(f'Flipped image {i+1}')\n        plt.axis('off')\n    plt.show()\n\n# Retrieve a batch of images from the training dataset and visualize the first one\nfor images, _ in train_dataset.take(1):\n    visualize_random_flip(images[0])\n\n\n\n\n\n\nRandomRotation Augmentation\n\ndef visualize_random_rotation(image):\n    random_rotation_layer = tf.keras.layers.RandomRotation(0.2)  # 20% of 360 degrees\n\n    plt.figure(figsize=(10, 10))\n    plt.subplot(3, 3, 1)\n    plt.imshow(image.numpy().astype(\"uint8\"))\n    plt.title('Original image')\n    plt.axis('off')\n    # Apply the RandomRotation layer to the same image several times\n    for i in range(8):\n        plt.subplot(3, 3, i+2)\n        rotated_image = random_rotation_layer(image, training=True)\n        plt.imshow(rotated_image.numpy().astype(\"uint8\"))\n        plt.title(f'Rotated image {i+1}')\n        plt.axis('off')\n    plt.show()\n\n# visualize the first one\nvisualize_random_rotation(images[0])\n\n\n\n\n\n\nBuild a Model with Data Augmentation\nNow we are creating a new model called model2 in which the first two layers are augmentation layers, while other layers remain similar to model1.\n\n# Create the Sequential model\nmodel2 = tf.keras.Sequential([\n    # Data Augmentation Block\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.2),\n\n    # First Convolutional Block\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Second Convolutional Block\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Third Convolutional Block\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n\n    # Classifier\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(2)\n])\n\n\n\nTrain and Visualize the Result\n\nhistory2 = compile_and_train(model2, train_dataset, validation_dataset)\n\nEpoch 1/20\n63/63 [==============================] - 13s 127ms/step - loss: 1.2798 - accuracy: 0.5445 - val_loss: 3.9599 - val_accuracy: 0.4802\nEpoch 2/20\n63/63 [==============================] - 8s 130ms/step - loss: 0.9775 - accuracy: 0.5725 - val_loss: 0.8683 - val_accuracy: 0.5656\nEpoch 3/20\n63/63 [==============================] - 9s 139ms/step - loss: 0.8225 - accuracy: 0.6055 - val_loss: 0.6811 - val_accuracy: 0.6188\nEpoch 4/20\n63/63 [==============================] - 9s 129ms/step - loss: 0.7024 - accuracy: 0.6385 - val_loss: 0.6933 - val_accuracy: 0.6312\nEpoch 5/20\n63/63 [==============================] - 9s 139ms/step - loss: 0.6925 - accuracy: 0.6445 - val_loss: 0.6573 - val_accuracy: 0.6151\nEpoch 6/20\n63/63 [==============================] - 8s 126ms/step - loss: 0.6302 - accuracy: 0.6770 - val_loss: 0.6138 - val_accuracy: 0.6770\nEpoch 7/20\n63/63 [==============================] - 9s 133ms/step - loss: 0.6284 - accuracy: 0.6745 - val_loss: 0.6516 - val_accuracy: 0.6658\nEpoch 8/20\n63/63 [==============================] - 10s 148ms/step - loss: 0.5832 - accuracy: 0.7135 - val_loss: 0.5679 - val_accuracy: 0.6955\nEpoch 9/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.5760 - accuracy: 0.7070 - val_loss: 0.8166 - val_accuracy: 0.6275\nEpoch 10/20\n63/63 [==============================] - 9s 131ms/step - loss: 0.5764 - accuracy: 0.7155 - val_loss: 0.5684 - val_accuracy: 0.7054\nEpoch 11/20\n63/63 [==============================] - 9s 133ms/step - loss: 0.5581 - accuracy: 0.7175 - val_loss: 0.6261 - val_accuracy: 0.6448\nEpoch 12/20\n63/63 [==============================] - 8s 126ms/step - loss: 0.5315 - accuracy: 0.7420 - val_loss: 0.6335 - val_accuracy: 0.6535\nEpoch 13/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.5376 - accuracy: 0.7365 - val_loss: 0.5897 - val_accuracy: 0.7104\nEpoch 14/20\n63/63 [==============================] - 8s 130ms/step - loss: 0.5376 - accuracy: 0.7295 - val_loss: 0.8979 - val_accuracy: 0.6077\nEpoch 15/20\n63/63 [==============================] - 8s 129ms/step - loss: 0.5391 - accuracy: 0.7385 - val_loss: 0.6124 - val_accuracy: 0.6955\nEpoch 16/20\n63/63 [==============================] - 9s 128ms/step - loss: 0.5107 - accuracy: 0.7595 - val_loss: 0.6661 - val_accuracy: 0.6498\nEpoch 17/20\n63/63 [==============================] - 9s 140ms/step - loss: 0.5144 - accuracy: 0.7420 - val_loss: 0.5801 - val_accuracy: 0.7240\nEpoch 18/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.4859 - accuracy: 0.7755 - val_loss: 0.5417 - val_accuracy: 0.7401\nEpoch 19/20\n63/63 [==============================] - 8s 129ms/step - loss: 0.4973 - accuracy: 0.7630 - val_loss: 0.6247 - val_accuracy: 0.6943\nEpoch 20/20\n63/63 [==============================] - 9s 148ms/step - loss: 0.4820 - accuracy: 0.7625 - val_loss: 0.5458 - val_accuracy: 0.7228\n\n\n\nplot_training_history(history2)\n\n\n\n\nAs we can observe, the training accuracy goes to 0.77, about 0.2 lower than model1, but the validation accuracy becomes higher than before, reaching a maximum of 0.74. This shows that data augmentation has helped us balance the bias of the model, but the model is still not sufficient to make accuracy predictions, though it is 24% higher in accuracy than random guess.\n\n\n\n3. Model with Data Preprocessing\nSometimes, it can be helpful to make simple transformations to the input data. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.\nIn the following section, we will define a function to create a preprocessor for the input, and incoporate this into our model.\n\n# Define the preprocessor as the first layer of the model\ndef create_preprocessor():\n    i = tf.keras.Input(shape=(160, 160, 3))\n    x = tf.keras.applications.mobilenet_v2.preprocess_input(i)\n    preprocessor = tf.keras.Model(inputs=[i], outputs=[x])\n    return preprocessor\n\n# Create the model with the preprocessor layer, data augmentation, and the rest of the architecture\ndef create_model3():\n    preprocessor = create_preprocessor()\n    model = tf.keras.Sequential([\n        preprocessor,\n        tf.keras.layers.RandomFlip(\"horizontal\"),\n        tf.keras.layers.RandomRotation(0.1),\n\n        # First Convolutional Block\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Dropout(0.25),\n\n        # Second Convolutional Block\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Dropout(0.25),\n\n        # Third Convolutional Block\n        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n        tf.keras.layers.Dropout(0.25),\n\n        # Classifier\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(2)\n    ])\n    return model\n\n\n# Build model, train, and visualize the result\nmodel3 = create_model3()\nhistory3 = compile_and_train(model3, train_dataset, validation_dataset)\nplot_training_history(history3)\n\nEpoch 1/20\n63/63 [==============================] - 13s 154ms/step - loss: 1.2568 - accuracy: 0.5475 - val_loss: 2.8958 - val_accuracy: 0.4950\nEpoch 2/20\n63/63 [==============================] - 8s 122ms/step - loss: 0.8847 - accuracy: 0.6115 - val_loss: 0.8993 - val_accuracy: 0.5087\nEpoch 3/20\n63/63 [==============================] - 9s 133ms/step - loss: 0.7476 - accuracy: 0.6495 - val_loss: 2.0710 - val_accuracy: 0.4839\nEpoch 4/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.6810 - accuracy: 0.6555 - val_loss: 2.4050 - val_accuracy: 0.4950\nEpoch 5/20\n63/63 [==============================] - 8s 124ms/step - loss: 0.6590 - accuracy: 0.6835 - val_loss: 1.4140 - val_accuracy: 0.5025\nEpoch 6/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.6129 - accuracy: 0.6980 - val_loss: 1.4951 - val_accuracy: 0.5483\nEpoch 7/20\n63/63 [==============================] - 10s 148ms/step - loss: 0.5817 - accuracy: 0.7125 - val_loss: 1.1408 - val_accuracy: 0.5582\nEpoch 8/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.5568 - accuracy: 0.7370 - val_loss: 0.7060 - val_accuracy: 0.6782\nEpoch 9/20\n63/63 [==============================] - 8s 127ms/step - loss: 0.5054 - accuracy: 0.7525 - val_loss: 0.6921 - val_accuracy: 0.6547\nEpoch 10/20\n63/63 [==============================] - 10s 151ms/step - loss: 0.5229 - accuracy: 0.7615 - val_loss: 0.7175 - val_accuracy: 0.6559\nEpoch 11/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.5293 - accuracy: 0.7420 - val_loss: 0.6566 - val_accuracy: 0.6955\nEpoch 12/20\n63/63 [==============================] - 8s 131ms/step - loss: 0.4943 - accuracy: 0.7600 - val_loss: 0.5879 - val_accuracy: 0.7240\nEpoch 13/20\n63/63 [==============================] - 9s 139ms/step - loss: 0.4684 - accuracy: 0.7895 - val_loss: 0.5879 - val_accuracy: 0.7116\nEpoch 14/20\n63/63 [==============================] - 8s 124ms/step - loss: 0.4616 - accuracy: 0.7850 - val_loss: 0.7305 - val_accuracy: 0.6733\nEpoch 15/20\n63/63 [==============================] - 8s 129ms/step - loss: 0.4520 - accuracy: 0.7845 - val_loss: 0.7629 - val_accuracy: 0.6436\nEpoch 16/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.4314 - accuracy: 0.7950 - val_loss: 0.5917 - val_accuracy: 0.7438\nEpoch 17/20\n63/63 [==============================] - 8s 123ms/step - loss: 0.4302 - accuracy: 0.7990 - val_loss: 0.5291 - val_accuracy: 0.7611\nEpoch 18/20\n63/63 [==============================] - 8s 128ms/step - loss: 0.4209 - accuracy: 0.8135 - val_loss: 0.6225 - val_accuracy: 0.7030\nEpoch 19/20\n63/63 [==============================] - 9s 145ms/step - loss: 0.4149 - accuracy: 0.8135 - val_loss: 0.6208 - val_accuracy: 0.7265\nEpoch 20/20\n63/63 [==============================] - 10s 141ms/step - loss: 0.4416 - accuracy: 0.7900 - val_loss: 0.6819 - val_accuracy: 0.7215\n\n\n\n\n\nThere is a constant increase in training and validation accuracy, where the training accuracy increases less sharply after reaching 0.78, and validation accuracy fluctuates around 0.7. Though the training accuracy is still higher than the validation accuracy, the overfitting problem is less severe than the previous two models. As the training going on, the training and validation accuracy increase at the same time.\n\n\n4. Model with Tansfer Learning\nSo far we are building everything and train randomized weights from scratch. But the idea of transfer learning allows us to make use of some pretrained models that does a related task. For example, a model that is trained on imagenet could help us locate features in a picture so that we only need to add a classifier and fine-tune the whole model to get the results.\n\ndef create_model4():\n    # Download and configure the MobileNetV2 base model\n    base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n                                                  include_top=False,\n                                                  weights='imagenet')\n    base_model.trainable = False\n\n    # Define the base_model_layer using the base model\n    i = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n    x = base_model(i, training=False)\n    base_model_layer = tf.keras.Model(inputs=[i], outputs=[x])\n\n    # Create the preprocessor layer\n    preprocessor = create_preprocessor()  # Assuming this function was defined as before\n\n    # Define the new model using the layers described\n    model4 = tf.keras.Sequential([\n        preprocessor,\n        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n        tf.keras.layers.RandomRotation(0.2),\n        base_model_layer,\n        tf.keras.layers.GlobalMaxPooling2D(),\n        tf.keras.layers.Dropout(0.2),  # Optional, adjust the rate as needed\n        tf.keras.layers.Dense(2)  # Output layer with 2 neurons for binary classification\n    ])\n    return model4\n\n\nmodel4 = create_model4()\nhistory4 = compile_and_train(model4, train_dataset, validation_dataset)\n# Visualize the training history\nplot_training_history(history4)\n\nEpoch 1/20\n63/63 [==============================] - 13s 155ms/step - loss: 1.2855 - accuracy: 0.7155 - val_loss: 0.1718 - val_accuracy: 0.9455\nEpoch 2/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.5989 - accuracy: 0.8450 - val_loss: 0.1537 - val_accuracy: 0.9480\nEpoch 3/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.5038 - accuracy: 0.8670 - val_loss: 0.1679 - val_accuracy: 0.9480\nEpoch 4/20\n63/63 [==============================] - 5s 84ms/step - loss: 0.4607 - accuracy: 0.8830 - val_loss: 0.1174 - val_accuracy: 0.9629\nEpoch 5/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.4095 - accuracy: 0.8925 - val_loss: 0.0826 - val_accuracy: 0.9715\nEpoch 6/20\n63/63 [==============================] - 5s 81ms/step - loss: 0.3745 - accuracy: 0.8950 - val_loss: 0.1055 - val_accuracy: 0.9567\nEpoch 7/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.3485 - accuracy: 0.9015 - val_loss: 0.1217 - val_accuracy: 0.9530\nEpoch 8/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.3826 - accuracy: 0.8980 - val_loss: 0.0818 - val_accuracy: 0.9728\nEpoch 9/20\n63/63 [==============================] - 6s 91ms/step - loss: 0.3408 - accuracy: 0.9095 - val_loss: 0.1318 - val_accuracy: 0.9567\nEpoch 10/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.4176 - accuracy: 0.8990 - val_loss: 0.0917 - val_accuracy: 0.9715\nEpoch 11/20\n63/63 [==============================] - 6s 96ms/step - loss: 0.2995 - accuracy: 0.9120 - val_loss: 0.0858 - val_accuracy: 0.9740\nEpoch 12/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.3142 - accuracy: 0.9090 - val_loss: 0.0775 - val_accuracy: 0.9790\nEpoch 13/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.3440 - accuracy: 0.8960 - val_loss: 0.0750 - val_accuracy: 0.9790\nEpoch 14/20\n63/63 [==============================] - 5s 82ms/step - loss: 0.2690 - accuracy: 0.9095 - val_loss: 0.1389 - val_accuracy: 0.9530\nEpoch 15/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.2928 - accuracy: 0.9130 - val_loss: 0.0719 - val_accuracy: 0.9728\nEpoch 16/20\n63/63 [==============================] - 6s 91ms/step - loss: 0.2924 - accuracy: 0.9095 - val_loss: 0.0744 - val_accuracy: 0.9728\nEpoch 17/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.2639 - accuracy: 0.9130 - val_loss: 0.0727 - val_accuracy: 0.9703\nEpoch 18/20\n63/63 [==============================] - 5s 70ms/step - loss: 0.2810 - accuracy: 0.9060 - val_loss: 0.0773 - val_accuracy: 0.9728\nEpoch 19/20\n63/63 [==============================] - 4s 60ms/step - loss: 0.2505 - accuracy: 0.9185 - val_loss: 0.0974 - val_accuracy: 0.9703\nEpoch 20/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.2911 - accuracy: 0.9160 - val_loss: 0.0816 - val_accuracy: 0.9728\n\n\n\n\n\nThe validation accuracy is consistantly above 0.97, and the training accuracy is around 0.9. The accuracy is significantly higher than the previous three models, and overfitting is not observed.\n\nmodel4.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_2 (Functional)        (None, 160, 160, 3)       0         \n                                                                 \n random_flip_3 (RandomFlip)  (None, 160, 160, 3)       0         \n                                                                 \n random_rotation_3 (RandomR  (None, 160, 160, 3)       0         \n otation)                                                        \n                                                                 \n model_1 (Functional)        (None, 5, 5, 1280)        2257984   \n                                                                 \n global_max_pooling2d (Glob  (None, 1280)              0         \n alMaxPooling2D)                                                 \n                                                                 \n dropout_12 (Dropout)        (None, 1280)              0         \n                                                                 \n dense_6 (Dense)             (None, 2)                 2562      \n                                                                 \n=================================================================\nTotal params: 2260546 (8.62 MB)\nTrainable params: 2562 (10.01 KB)\nNon-trainable params: 2257984 (8.61 MB)\n_________________________________________________________________\n\n\nFrom the model summary, we can see that most of the parameters are from the pretrined model within the base_model_layer."
  },
  {
    "objectID": "posts/Dog Cat classification/Dog_cat_classfication.html#evaluate-best-model-on-testset",
    "href": "posts/Dog Cat classification/Dog_cat_classfication.html#evaluate-best-model-on-testset",
    "title": "Dog Cat Classfication with Tensorflow",
    "section": "Evaluate Best Model on Testset",
    "text": "Evaluate Best Model on Testset\n\n# Evaluate on test set\ntest_loss, test_accuracy = model4.evaluate(test_dataset)\n\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n6/6 [==============================] - 1s 88ms/step - loss: 0.0464 - accuracy: 0.9740\nTest Accuracy: 97.40%\n\n\nAs a result, the model gets 97.40% accuracy on the unseen testset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Dog Cat Classfication with Tensorflow\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nHaoran Jia\n\n\n\n\n\n\n  \n\n\n\n\nWeb scraping with Scrapy\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nHaoran Jia\n\n\n\n\n\n\n  \n\n\n\n\nDatabase Queries and Interactive Visualizations with Python\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\nHaoran Jia\n\n\n\n\n\n\n  \n\n\n\n\nStart With Quarto\n\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nHaoran Jia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, my name is Haoran Jia, and I am a senior student at UCLA. My interest lies in data science and machine learning. This website will host some of simple tutorials playing with Python."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA\nMajor in Applied Mathematics, Specialization in Computing | Sept 2020 - June 2024"
  },
  {
    "objectID": "posts/Database Queries and Interactive Visualizations with Python/index.html",
    "href": "posts/Database Queries and Interactive Visualizations with Python/index.html",
    "title": "Database Queries and Interactive Visualizations with Python",
    "section": "",
    "text": "Source: https://plotly.com/about-us/\n\n\nIn this tutorial, we’ll work with sqlite3, plotly, and pandas on a climate dataset. By the end, you should be comfortable with creating a SQLite database using Python, querying with SQL, and crafting customized graphs with Plotly.\nNote: For the successful rendering of the interactive plots, run the following.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nData Import\nFirst, we need to read in three required tables with pandas, and then we will be creating a database based on these tables.\n\nimport pandas as pd\n\n# Read from files\nstations = pd.read_csv(\"station-metadata.csv\")\ntemps = pd.read_csv('temps_stacked.csv')\ncountries = pd.read_csv('countries.csv')\n\nLet’s take a look at each dataset.\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\ntemps.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\n\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nNotice that there is no country names in stations. We may want to add an extra column named country to it for future convenience. We can do this by merging stations and countries using the two-letter ID. As a good practice, all operations with dataframe will be enclosed in a function.\n\ndef prepare_df(stations):\n    \"\"\"\n    Prepare the stations DataFrame by merging it with the countries DataFrame and renaming columns.\n\n    This function extracts the 'FIPS 10-4' code from the 'ID' column of the stations DataFrame,\n    merges the stations DataFrame with the countries DataFrame based on the 'FIPS 10-4' code,\n    drops unnecessary columns, and renames the 'Name' column to 'Country'.\n\n    Parameters:\n    - stations (pd.DataFrame): The stations dataset.\n\n    Returns:\n    - pd.DataFrame: A modified version of the stations DataFrame after merging and renaming operations.\n    \"\"\"\n\n    # Extract the 'FIPS 10-4' code from the 'ID' column\n    stations['FIPS 10-4'] = stations['ID'].str[:2]\n\n    # Merge the stations DataFrame with the countries DataFrame based on the 'FIPS 10-4' code\n    stations = pd.merge(stations, countries, on='FIPS 10-4')\n\n    # Drop unnecessary columns\n    stations = stations.drop(['FIPS 10-4', 'ISO 3166'], axis=1)\n\n    # Rename the 'Name' column to 'Country'\n    stations.rename(columns={'Name': 'Country'}, inplace=True)\n\n    return stations\n\nstations = prepare_df(stations)\n\n\n\nCreating Databases\nIn this section, we are going to create a database with three tables we have: temperatures, stations, and countries.\n\nimport sqlite3\n\n# Create database\nconn = sqlite3.connect(\"temps.db\")\n\nWe observe that temps is extremely large, so we may want to add it to the database by chunks. We can write a function to do this.\n\ndef read_csv_to_database(csv_file_path, database_connection):\n    # Read segmented table into database\n    df_iter = pd.read_csv(csv_file_path, chunksize=100000)\n    for df in df_iter:\n        df.to_sql(\"temperatures\", database_connection, if_exists=\"append\", index=False)\n\ncsv_file_path = 'temps_stacked.csv'\nread_csv_to_database(csv_file_path, conn)\n\nAdd other two datasets to the database.\n\n# Read other two tables into database\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\n279\n\n\nAlways remember to close the dataset connection when you are not using.\n\nconn.close()\n\n\n\nA simple Query Function\nConnect to the database before using, and then create a cursor to interact with it.\n\n# connect to database\nconn = sqlite3.connect(\"temps.db\")\n\nLet’s first check all three tables are in the database.\n\n# Create a cursor object to interact with the SQLite database\ncursor = conn.cursor()\n\n# Execute an SQL query to fetch the names of all tables in the database\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n\n# Print the names of the tables\nprint(cursor.fetchall())\n\n# Close the database connection\nconn.close()\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nWith the data, we can start to ask our first question: how does the average yearly change in temperature vary within a given country?\nWe may create a map scatter plot where color of dots will represent temperature. But before doing that, we may want to retrieve necessary data frist.\n\nWe are going to do this with a query function called query_climate_database which accepts four arguments:\n\n\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned.\nmonth, an integer giving the month of the year for which should be returned.\n\nThe return value is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year.\n\ndef query_climate_database(country, year_begin, year_end, month):\n    \"\"\"\n    Fetch climate data for a specified country, year range, and month from an SQLite database.\n\n    This function queries temperature data based on a specified country, year range, and month\n    from an SQLite database and returns the data as a pandas DataFrame.\n\n    Parameters:\n    - country (str): The name of the country for which data is to be fetched.\n    - year_begin (int): The starting year for the data range.\n    - year_end (int): The ending year for the data range.\n    - month (int): The month for which data is to be fetched.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing temperature data for the specified country, year range, and month.\n\n    Example:\n    &gt;&gt;&gt; df = query_climate_database('India', 2000, 2020, 1)\n    Fetches temperature data for India for the month of January between the years 2000 and 2020.\n    \"\"\"\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(\"temps.db\")\n    cursor = conn.cursor()\n\n    # Execute the SQL query to fetch temperature data\n    cursor.execute(f\"SELECT s.NAME, s.LATITUDE, s.LONGITUDE, s.Country, t.Year, t.Month, t.Temp \\\n                     FROM temperatures t JOIN stations s ON s.ID = t.ID \\\n                     WHERE s.Country = '{country}' AND Year &gt;= {year_begin} AND Year &lt;= {year_end} AND t.Month = {month}\")\n\n    # Convert the fetched data to a pandas DataFrame\n    result_df = pd.DataFrame(cursor.fetchall(), columns=['Name', 'Latitude', 'Longitude',\n                                                         'Country', 'Year', 'Month', 'Temp'])\n\n    # Close the database connection\n    conn.close()\n\n    return result_df\n\nLet’s see an example usage.\n\n# Example output\nquery_climate_database(country = \"India\",\n                       year_begin = 1980,\n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12603\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n12604\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n12605\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n12606\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n12607\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n12608 rows × 7 columns\n\n\n\n\n\nVisualization 1: Geographic Scatter Plot for Yearly Temperature Increases\nBefore moving, let’s first compute a simple estimate of the year-over-year average change in temperature in each month at each station to add more fun. For this, we’ll use our old friend, linear regression. We’ll use the statistical fact that, when regressing Temp against Year, the coefficient of Year will be an estimate of the yearly change in Temp.\n\nfrom sklearn.linear_model import LinearRegression\n\ndef coef(data_group):\n    \"\"\"\n    Calculate the coefficient of the linear regression model for temperature against year.\n\n    This function fits a linear regression model using the \"Year\" as the independent variable\n    and \"Temp\" as the dependent variable. It returns the coefficient of the \"Year\",\n    which represents the rate of change of temperature with respect to the year.\n\n    Parameters:\n    - data_group: A pandas grouped object.\n\n    Returns:\n    - float: The coefficient of the \"Year\" in the linear regression model.\n    \"\"\"\n\n    # Extract x and y variables from data group\n    x = data_group[[\"Year\"]]\n    y = data_group[\"Temp\"]\n\n    # Initialize and fit the linear regression model\n    LR = LinearRegression()\n    LR.fit(x, y)\n\n    # Return the coefficient of the \"Year\"\n    return LR.coef_[0]\n\nNow we are going to write a function called temperature_coefficient_plot to visualize the data we fetched from the database. This function will accept five explicit arguments, and an undetermined number of keyword arguments.\n\ncountry, year_begin, year_end, and month are as in the previous part. min_obs, the minimum required number of years of data for any given station. Only data for stations with at least min_obs years worth of data in the specified month would be plotted; the others would be filtered out. We will use df.transform() plus filtering to achieve this task. **kwargs is an additional keyword arguments passed to px.scatter_mapbox(). These can be used to control the colormap used, the mapbox style, etc.\n\nThe output of this function would be an interactive geographic scatterplot, constructed using Plotly Express, with a point for each station, such that the color of the point reflects an estimate of the yearly change in temperature during the specified month and time period at that station.\n\nfrom plotly import express as px\n\n# Define a color map for the plot\ncolor_map = px.colors.diverging.RdGy_r\n# Create a mapping for creating titles\nmonth_mapping = {\n    1: \"January\",\n    2: \"February\",\n    3: \"March\",\n    4: \"April\",\n    5: \"May\",\n    6: \"June\",\n    7: \"July\",\n    8: \"August\",\n    9: \"September\",\n    10: \"October\",\n    11: \"November\",\n    12: \"December\"\n}\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Generate a scatter map plot showing the estimated yearly increase in temperature for various stations.\n\n    This function queries temperature data for a specified country, month, and year range.\n    It then calculates the coefficient of the linear regression model for temperature against year\n    for each station and plots the stations on a map. The color of each station represents the\n    estimated yearly increase in temperature.\n\n    Parameters:\n    - country (str): The name of the country for which the plot is to be generated.\n    - year_begin (int): The starting year for the data range.\n    - year_end (int): The ending year for the data range.\n    - month (int): The month for which the plot is to be generated.\n    - min_obs (int): Minimum number of observations required for a station to be included in the plot.\n    - **kwargs: Additional keyword arguments for the plotly express scatter_mapbox function.\n\n    Returns:\n    - plotly.graph_objs._figure.Figure: A Plotly figure object representing the scatter map plot.\n\n    Example:\n    &gt;&gt;&gt; fig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, min_obs=10, zoom=3, center={'lat':23,'lon':80})\n    Generates a scatter map plot for India for the month of January between the years 1980 and 2020.\n    \"\"\"\n\n    # Query the climate database for the specified parameters\n    temp_df = query_climate_database(country, year_begin, year_end, month)\n\n    # Filter stations based on the minimum number of observations\n    temp_df['min_obs'] = temp_df.groupby('Name')['Year'].transform('nunique')\n    temp_df = temp_df[temp_df['min_obs'] &gt;= min_obs]\n\n    # Calculate the coefficient for each station and merge it into the main dataframe\n    coefs = temp_df.groupby([\"Name\"]).apply(coef)\n    coefs = coefs.reset_index()\n    temp_df = pd.merge(temp_df, coefs, on='Name')\n    temp_df.rename(columns={0: 'Estimate'}, inplace=True)\n    temp_df['Estimate'] = temp_df['Estimate'].round(4)  # Round the estimates for better visualization\n    max_abs_estimate = temp_df['Estimate'].abs().max()  # Determine the maximum absolute estimate for color scaling\n\n    # Create the scatter map plot using plotly express\n    fig = px.scatter_mapbox(temp_df,\n                            hover_name=\"Name\",\n                            lat=\"Latitude\",\n                            lon=\"Longitude\",\n                            color='Estimate',\n                            range_color=[-max_abs_estimate, max_abs_estimate],\n                            **kwargs)\n\n    # Update the layout and title of the plot\n    fig.update_layout(title_text=f\"\"\"Estimate of yearly increase in temperature in {month_mapping[month]} for stations in {country}, between {year_begin} to {year_end}\"\"\")\n\n    return fig\n\nLet’s try generating a scatter map plot of temperature for India in January between the years 1980 and 2020.\n\n# Example usage: Generate and display the plot for India\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, min_obs=10,\n                                  zoom=3,\n                                  center={'lat':23,'lon':80},\n                                  mapbox_style=\"carto-positron\",\n                                  color_continuous_scale=color_map)\nfig.show()\n\n\n\n\nFrom this graph we can see that the estimate in temperature increase mostly appears near the border lines of India, while inland temperature is expected to drop.\nTo generalize the fucntion, let’s test with an another example.\n\nfig = temperature_coefficient_plot(\"China\", 1960, 1980, 10, min_obs = 10,\n                                  zoom = 2.5,\n                                  center = {'lat':40,'lon':110},\n                                  mapbox_style=\"carto-positron\",\n                                  color_continuous_scale=color_map)\nfig.show()\n\n\n\n\nThe graphs shows that the stations mostly centered at the South Eastern side of China, while the largest estimate in temperature increase appears in far North and West.\n\n\nVisualization 2: Line Plot for Yearly and Monthly Average Temperature Change\nIn this section, we want to ask: how does the average temperature in certain country changes over time?\nTo answer this question, we may create a line plot where x axis is date and y axis is the average temperature.\n\nAgain, we are going to start with a query function called query_avg_temp_by_country which accepts three arguments: country, year_begin, and year_end as in the previous part.\n\nThe return value is a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month of the year.\n\ndef query_avg_temp_by_country(country, year_begin=1901, year_end=2020):\n    \"\"\"\n    Fetch average temperature data for a specified country from an SQLite database.\n\n    This function queries temperature data based on a specified country and a year range\n    from an SQLite database and returns the data as a pandas DataFrame.\n\n    Parameters:\n    - country (str): The name of the country for which data is to be fetched.\n    - year_begin (int, optional): The starting year for the data range. Default is 1901.\n    - year_end (int, optional): The ending year for the data range. Default is 2020.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing temperature data for the specified country and year range.\n\n    Example:\n    &gt;&gt;&gt; df = query_avg_temp_by_country('India', year_begin=2000, year_end=2010)\n    Fetches temperature data for India between the years 2000 and 2010.\n    \"\"\"\n\n    # Connect to the SQLite database\n    conn = sqlite3.connect(\"temps.db\")\n    cursor = conn.cursor()\n\n    # Execute the SQL query to fetch temperature data\n    cursor.execute(f\"SELECT s.Country, t.Year, t.Month, t.Temp \\\n                     FROM temperatures t JOIN stations s ON s.ID = t.ID \\\n                     WHERE s.Country = '{country}' AND t.Year &gt;= {year_begin} AND t.Year &lt;= {year_end}\")\n\n    # Convert the fetched data to a pandas DataFrame\n    result_df = pd.DataFrame(cursor.fetchall(), columns=['Country', 'Year', 'Month', 'Temp'])\n\n    # Close the database connection\n    conn.close()\n\n    return result_df\n\nThen similarly, we will create a user-friendly function called temp_trend_lineplot to help us with the plotting. The function is supposed to takes in country, year_begin, and year_end as in the previous part, and returns a line plot show the monthly and yearly change in average temperature.\nBut beside the baseline, we can have some addition to the interactive plot. We will create two seperate figures for monthly and yearly average temperature, and adds bottons to control the displayed time period.\nIn the actual inplementation, we need to process the data using groupby() to get the average for a specific time period and to_datetime function to convert the time into datetime object for plotting.\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef temp_trend_lineplot(country, year_begin=1901, year_end=2020):\n    \"\"\"\n    Generate a time series line plot showing the monthly and yearly average temperature for a specified country.\n\n    Parameters:\n    - country (str): The name of the country for which the plot is to be generated.\n    - year_begin (int, optional): The starting year for the data range. Default is 1901.\n    - year_end (int, optional): The ending year for the data range. Default is 2020.\n\n    Returns:\n    - A Plotly figure object representing the time series line plot.\n\n    Example:\n    &gt;&gt;&gt; fig = temp_trend_lineplot('India')\n    Generates a line plot for India showing monthly and yearly average temperatures.\n    \"\"\"\n\n    # Fetch average temperature data for the specified country and year range\n    result_df = query_avg_temp_by_country(country, year_begin, year_end)\n\n    # Calculate monthly average temperatures\n    avgs_df = result_df.groupby(['Year', 'Month'])['Temp'].mean().reset_index()\n    avgs_df['Date'] = pd.to_datetime(avgs_df['Year'].astype(str) + '-' + avgs_df['Month'].astype(str) + '-01')\n\n    # Calculate yearly average temperatures\n    yearly_avg = avgs_df.resample('Y', on='Date').mean()\n\n    # Create subplots for both monthly and yearly averages\n    fig = make_subplots(rows=2, cols=1,\n                        shared_xaxes=True, vertical_spacing=0.1,\n                        subplot_titles=('Monthly Average', 'Yearly Average'))\n\n    # Add trace for monthly average temperatures\n    fig.add_trace(\n        go.Scatter(x=avgs_df.Date, y=avgs_df.Temp, name='Monthly Average'),\n        row=1, col=1)\n\n    # Add trace for yearly average temperatures\n    fig.add_trace(\n        go.Scatter(x=yearly_avg.index, y=yearly_avg.Temp, name='Yearly Average'),\n        row=2, col=1)\n\n    # Set the title and layout properties for the plot\n    fig.update_layout(\n        title_text=f\"Time series average temperature in {country}\",\n        height=800\n    )\n\n    # Add a range slider for date selection\n    fig.update_layout(\n        xaxis=dict(\n            rangeselector=dict(\n                buttons=list([\n                    dict(count=6,\n                         label=\"6m\",\n                         step=\"month\",\n                         stepmode=\"backward\"),\n                    dict(count=1,\n                         label=\"1y\",\n                         step=\"year\",\n                         stepmode=\"backward\"),\n                    dict(count=5,\n                         label=\"5y\",\n                         step=\"year\",\n                         stepmode=\"backward\"),\n                    dict(step=\"all\")\n                ])\n            ),\n            rangeslider=dict(\n                visible=False\n            ),\n            type=\"date\"\n        )\n    )\n    return fig\n\n\n# Example usage: Generate and display the plot for India\nfig = temp_trend_lineplot('India')\nfig.show()\n\n\n\n\nIt seems that the seasonal temperature change are pretty regular in the long term, but in general, the yearly average temperature plot suggests that the temperature in India keeps getting warm from 1901 to recent years, though with some fluctuations.\nSee an another example.\n\nfig = temp_trend_lineplot('United States', year_begin = 2000, year_end = 2020)\nfig.show()\n\n\n\n\nThe seasonal pattern is also clear in the US, but it fluctuates at a much lower level compared with India, from around 0 to 25, while India’s temperature varies from around 16 to 30.\n\n\nVisualization 3: Stacked Histogram With Box Plot for Temperature Distribution\nIn this section, we want to answer the question: how does the distribution of temperature differ between different countries?\nAs usual, the first task is to create a query function to retrieve the data. We will create a function called query_temp_by_country that takes four parameters, one required list-type argument countries, and three optional arguments: month,year_begin, year_end. If month is not specified, then it will return all temperature data within the time period. The return would be a DataFrame containing temperature data for the specified countries, month, and year range.\n\ndef query_temp_by_country(countries, month=0, year_begin=1901, year_end=2020):\n    \"\"\"\n    Fetch temperature data for specified countries from an SQLite database.\n\n    This function queries temperature data based on a list of countries, a specified month,\n    and a year range from the database. The data is returned as a pandas DataFrame.\n\n    Parameters:\n    - countries (list of str): A list of country names for which data is to be fetched.\n    - month (int, optional): The month for which data is to be fetched. Default is 0, which fetches data for all months.\n    - year_begin (int, optional): The starting year for the data range. Default is 1901.\n    - year_end (int, optional): The ending year for the data range. Default is 2020.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing temperature data for the specified countries, month, and year range.\n\n    Example:\n    &gt;&gt;&gt; query_temp_by_country(['Zimbabwe', 'USA'], month=1, year_begin=2000, year_end=2010)\n    Returns a DataFrame with temperature data for Zimbabwe and USA for the month of January between the years 2000 and 2010.\n    \"\"\"\n\n    conn = sqlite3.connect(\"temps.db\")\n    cursor = conn.cursor()\n\n    # Create a placeholder for each country in the list\n    placeholders = ', '.join(['?'] * len(countries))\n\n    # Using parameterized query for different input length\n    query_str = f\"\"\"SELECT t.Year, t.Temp, s.Country, t.Month\n                    FROM temperatures t JOIN stations s\n                    ON s.ID = t.ID\n                    WHERE s.Country IN ({placeholders}) AND t.Year &gt;= ? AND t.Year &lt;= ?\"\"\"\n\n    # List of parameters for the query\n    params = countries + [year_begin, year_end]\n\n    # Add restrictions to month depending on the input\n    if month &gt; 0:\n        query_str += ' AND t.Month = ?'\n        params.append(month)\n\n    # Query and get results\n    cursor.execute(query_str, params)\n    result_df = pd.DataFrame(cursor.fetchall(), columns=['Year', 'Temp', 'Country', 'Month'])\n    conn.close()\n\n    return result_df\n\nWith the query function, we can start working on the actual plotting function. We will create a function named stacked_histogram that takes in the same arguments as query_temp_by_country. Since the input is a bit different as previous functions, we hope to remind the user of the data type. In Python Raises...TypeError allows us to do this.\nAs return, we hope to get a stacked histogram for a list of countries, with their box plots, where the y axis suggest the number of observations, and x axis specifies temperature bins.\n\ndef stacked_histogram(countries, month=0, year_begin=1901, year_end=2020):\n    \"\"\"\n    Create a stacked histogram of temperature data for specified countries.\n\n    This function fetches temperature data based on a list of countries, a specified month,\n    and a year range using the `query_temp_by_country` function. It then creates a stacked histogram\n    using Plotly Express and returns the figure.\n\n    Parameters:\n    - countries (list of str): A list of country names for which the histogram is to be created.\n    - month (int, optional): The month for which the histogram is to be created. Default is 0, which uses data for all months.\n    - year_begin (int, optional): The starting year for the data range. Default is 1901.\n    - year_end (int, optional): The ending year for the data range. Default is 2020.\n\n    Returns:\n    - A Plotly figure object representing the stacked histogram.\n\n    Example:\n    &gt;&gt;&gt; fig = stacked_histogram(['Zimbabwe', 'United States'], month=1, year_begin=2000, year_end=2010)\n    Creates and returns a stacked histogram for Zimbabwe and USA for the month of January between the years 2000 and 2010.\n    \"\"\"\n    if type(countries) != list:\n        raise TypeError('Countries must be a list.')\n\n    # retrieve data from database\n    result_df = query_temp_by_country(countries, month, year_begin, year_end)\n    # plot a stacked histogram\n    fig = px.histogram(result_df, x=\"Temp\", color=\"Country\", marginal='box')\n    # add a title\n    title_text = f\"Stacked histogram of historical termperature distribution between {year_begin} and {year_end}\"\n    if month&gt;0:\n            title_text += f\" in {month_mapping[month]}\"\n    \n    # Set the title\n    fig.update_layout(\n        title_text=title_text\n    )\n\n    return fig\n\n\n# Let's try an extreme cases.\nstacked_histogram(['Zimbabwe','Netherlands'])\n\n\n\n\nWe can tell that the two countries have different climate patterns since their interquartile range did not overlap. Also, the height of the stacked chart suggest probably these two countries have similar numbers of stations.\n\n# One more example\nstacked_histogram(['Argentina', 'China','Canada'], month = 10)\n\n\n\n\nFrom this graph, we can tell that Canada have many extreme values below 0, as the outliers. China has the longest interquartile range, suggesting a larger variation in climate patterns within the country."
  },
  {
    "objectID": "posts/Start With Quarto/index.html",
    "href": "posts/Start With Quarto/index.html",
    "title": "Start With Quarto",
    "section": "",
    "text": "This is a simple tutorial of how we can read in an online dataset and create a simple visualizaion for Palmer Penguins dataset.\n\nData import\nWe need to first load in the dataset from online repository. We can use pandas to do this.\n\nimport pandas as pd\n# retrieve from online repository\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nHere we successfully read in the dataset, and then we can start building a simple visualization.\n\n\nPlotting\nWe will be using matplotlib for plotting since it offers us great freedom in customizing the graph.\n\nimport matplotlib.pyplot as plt\n\n# Create a scatterplot to visualize the relationship between Flipper length and culmen Length\nplt.scatter(x = penguins['Flipper Length (mm)'], y = penguins['Culmen Length (mm)'])\n# Add a title to the graph\nplt.title(\"Flipper Length VS Culmen Length\")\n# Name the axises\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\n\n\n\nThis is a scatter plot based on palmer penguins dataset. It plots the penguins’ flipper length against culmen length. In general, it shows a positive correlation between the two length."
  }
]